library(MASS)
th<-mvrnorm(N,mu=c(0,0),Sigma=matrix(c(1,r,r,1),2,2,byrow=TRUE))
#make first n items unidimensional
b1<-rnorm(n)
kern<-outer(th[,1],b1,"+") #this just creates a matrix consisting where element i,j is th[i,1]+b1[j]
kern<-exp(kern)
pv<-kern/(1+kern)
runif(N*n)->test
ifelse(pv>test,1,0)->resp1
#now second n items are multidimensional (if A>0) with increasing loadings on the second dimension
a<-seq(0,A,length.out=n)
b2<-rnorm(n)
mat<-list()
for (i in 1:N) th[i,1]+a*th[i,2]+b2->mat[[i]] #this is where the multidimensional magic happens
do.call("rbind",mat)->kern
kern<-exp(kern)
pv<-kern/(1+kern)
runif(N*n)->test
ifelse(pv>test,1,0)->resp2
resp<-cbind(resp1,resp2) #so first half of items will be unidimensional and second half multidimensional
1:ncol(resp)->colnames(resp)
list(true=list(a=a,b=c(b1,b2),th=th),resp=resp)
}
sim_md()->resp
##How does this depend on choice of item response model used for parameter recovery (not data generation, be sure to keep the distinction between these two front and center)?
##i will focus on the rasch and 2pl [note that i am not simulating any guessing]
par(mfrow=c(2,2),mgp=c(2,1,0),mar=c(3,3,1,1))
sim_md(N=2000,n=50,A=2)->resp
compare(resp,itemtype="Rasch")
sim_md(N=2000,n=50,A=2)->resp
compare(resp,itemtype="2PL")
par(mfrow=c(4,2),mgp=c(2,1,0),mar=c(3,3,1,1))
sim_md(N=2000,r=0,n=50,A=2)->resp
compare(resp,itemtype="Rasch")
sim_md(N=2000,r=1,n=50,A=2)->resp
compare(resp,itemtype="Rasch")
sim_md(N=2000,n=50,A=2)->resp
compare(resp,itemtype="2PL")
## ##For fun let's now estimate the right model
sim_md(N=5000,r=0)->resp
mirt(resp$resp,2,"2PL")->mod
coef(mod)->co
co[-length(co)]->co
do.call("rbind",co)->co
plot(co[,1],pch=19,ylim=c(-2,2))
points(co[,2],pch=19,col="red") #subtle things are afoot here. if you know anything about factor analysis, the reason this isn't quite what you expect is related to the idea of "rotation"
set.seed(12101)
compare<-function(resp,itemtype="2PL") {
##let's first estimate a **unidimensional** model. this is wrong, of course.
library(mirt)
mirt(resp$resp,1,itemtype=itemtype)->mod
coef(mod)->co
co[-length(co)]->co
do.call("rbind",co)->co
##now we create two plots
##first plot, looking at easiness: absolute bias in easiness estimates as a function of item position
##second plot, looking at discrimination: density of discrimination coefs separately for unidim items (black) and multidim items (red)
#
nrow(co)->n
plot(abs(co[,2]-resp$true$b),col=c(rep("black",n/2),rep("red",n/2)),pch=19,ylab=c("abs(est easy - true easy)"))
1:n->xv
abs(co[,2]-resp$true$b)->yv
loess(yv~xv)->m
lines(xv,m$fitted)
#
plot(density(co[1:n/2,1]),col="black",xlab="density of discrimination",xlim=c(0,2))
lines(density(co[(n/2+1):n,1]),col="red")
}
##How does this depend on choice of r?
par(mfrow=c(3,2),mgp=c(2,1,0),mar=c(3,3,1,1))
##How does this depend on choice of item response model used for parameter recovery (not data generation, be sure to keep the distinction between these two front and center)?
##i will focus on the rasch and 2pl [note that i am not simulating any guessing]
par(mfrow=c(2,2),mgp=c(2,1,0),mar=c(3,3,1,1))
sim_md(N=2000,n=50,A=2)->resp
compare(resp,itemtype="Rasch")
sim_md(N=2000,n=50,A=2)->resp
compare(resp,itemtype="2PL")
par(mfrow=c(4,2),mgp=c(2,1,0),mar=c(3,3,1,1))
sim_md(N=2000,r=0,n=50,A=2)->resp
compare(resp,itemtype="Rasch")
sim_md(N=2000,r=1,n=50,A=2)->resp
compare(resp,itemtype="Rasch")
sim_md(N=2000,n=50,A=2)->resp
compare(resp,itemtype="2PL")
## ##For fun let's now estimate the right model
sim_md(N=5000,r=0)->resp
mirt(resp$resp,2,"2PL")->mod
coef(mod)->co
co[-length(co)]->co
do.call("rbind",co)->co
plot(co[,1],pch=19,ylim=c(-2,2))
points(co[,2],pch=19,col="red") #subtle things are afoot here. if you know anything about factor analysis, the reason this isn't quite what you expect is related to the idea of "rotation"
th<-seq(-3,3,length.out=1000)
p<-function(b) exp(th-b)/(1+exp(th-b))
plot(th,p(-1)*p(0)*(1-p(+1.5)))
th<-seq(-3,3,length.out=1000)
p<-function(b) exp(th-b)/(1+exp(th-b))
plot(th,p(-1)*p(0)*(1-p(+1.5)))
plot(th,p(-1)*(1-p(0))*(1-p(+1.5)))
plot(th,p(-1)*(p(0))*(p(+1.5)))
plot(th, p(-1)*(1-p(-1)) + p(0) * (1-p(0)) + (1-p(+1.5))*p(1.5))
simdata<-function(ni,th) {
np<-length(th)
th.mat<-matrix(th,np,ni,byrow=FALSE)
a<-exp(rnorm(ni,sd=.3))
b<-rnorm(ni)
a.mat<-matrix(rep(a,np),np,ni,byrow=TRUE)
b.mat<-matrix(b,np,ni,byrow=TRUE)
inv_logit<-function(x) exp(x)/(1+exp(x))
pr<-inv_logit(a.mat*(th.mat+b.mat))
resp<-pr
for (i in 1:ncol(resp)) resp[,i]<-rbinom(nrow(resp),1,resp[,i])
resp<-data.frame(resp)
names(resp)<-paste("item",1:ni,sep='')
list(item=data.frame(a=a,b=b),resp=resp)
}
np<-1000
th<-runif(np,min=-3,max=3)
library(mirt)
L<-simdata(ni=25,th=th)
m<-mirt(L$resp,1,'2PL')
th.est<-fscores(m)[,1]
par(mfrow=c(1,3),mgp=c(2,1,0),mar=c(3,3,1,1))
hist(th)
hist(th.est)
plot(th,th.est); abline(0,1) #uhoh!
m<-mirt(L$resp,1,'2PL', technical = list(prior = list(mean = 0, sd = 1)))
th.est<-fscores(m)[,1]
prior <- list(
Theta = list(mu = 0, sigma = 1, nu = 1)
)
m<-mirt(L$resp,1,'2PL',prior)
th.est<-fscores(m)[,1]
model2 <- mirt.model('
Theta = 1-10
(Theta * Theta) = 2,4,6,8,10 ## quadratic factor
## constrain first factor slopes to be equal
CONSTRAIN = (1-10, a1)
## N(0,1) prior on d for item 2,3,5, N(0,0.5) for item 4
PRIOR = (2-3, 5, d, norm, 0, 1), (4, d, norm, 0, 0.5)')
m<-mirt(L$resp,model2,'2PL',prior)
m<-mirt(L$resp,model2,'2PL')
th.est<-fscores(m)[,1]
par(mfrow=c(1,3),mgp=c(2,1,0),mar=c(3,3,1,1))
hist(th)
hist(th.est)
plot(th,th.est); abline(0,1) #uhoh!
model2 <- mirt.model('
Theta = 1-10
## N(0,1) prior on d for item 2,3,5, N(0,0.5) for item 4
PRIOR = (2-3, 5, d, norm, 0, 1), (4, d, norm, 0, 0.5)')
m<-mirt(L$resp,model2,'2PL')
th.est<-fscores(m)[,1]
par(mfrow=c(1,3),mgp=c(2,1,0),mar=c(3,3,1,1))
hist(th)
hist(th.est)
plot(th,th.est); abline(0,1) #uhoh!
get_pr<-function(th,a,b,c) {
x<-a*(th-b)
c+(1-c)/(1+exp(-x))
}
f1<-function(th,resp,a,b,c) { #used in straight MLE
p<-get_pr(th,a,b,c)
-sum(resp*log(p)+(1-resp)*log(1-p))
}
rmse<-function(x,y) sqrt(mean((x-y)^2))
ni<-10
b<-rnorm(ni) #difficulties
a<-exp(rnorm(ni,mean=0,sd=.25)) #discriminations
c<-runif(ni,min=0,max=0)
th<-rnorm(100)
est.mle<-est.eap<-numeric()
for (i in 1:length(th)) {
##we'll use th to simulate data
p<-get_pr(th[i],a,b,c)
resp<-rbinom(length(p),1,p)
##old MLE estimation based on optimization of f1
est.mle[i]<-optim(0,f1,method='Brent',lower=-10,upper=10,a=a,b=b,c=c,resp=resp)$par
##EAP estimation!!
x<-seq(-5,5,length.out=1000)
n<-d<-numeric() #numerator and deminator here  ##https://www.rasch.org/rmt/rmt163i.htm
for (ii in 1:length(x)) {
x0<-x[ii]
p<-get_pr(x0,a,b,c)
p<-prod(p^resp*(1-p)^(1-resp))
n[ii]<-x0*dnorm(x0)*p
d[ii]<-dnorm(x0)*p
}
est.eap[i]<-sum(n)/sum(d)
}
df<-data.frame(th=th,est.mle=est.mle,est.eap=est.eap)
plot(df)
plot(df)
rmse(th,est.mle)
rmse(th,est.eap) ##what do you notice about the RMSEs?
plot(df); abline(0,1)
plot(th, est.eap)
plot(th, est.eap); abline(0,1)
plot(th, est.mle); abline(0,1)
for (i in 1:length(th)) {
##we'll use th to simulate data
p<-get_pr(th[i],a,b,c)
resp<-rbinom(length(p),1,p)
##old MLE estimation based on optimization of f1
est.mle[i]<-optim(0,f1,method='Brent',lower=-10,upper=10,a=a,b=b,c=c,resp=resp)$par
##EAP estimation!!
x<-seq(-5,5,length.out=1000)
n<-d<-numeric() #numerator and deminator here  ##https://www.rasch.org/rmt/rmt163i.htm
for (ii in 1:length(x)) {
x0<-x[ii]
p<-get_pr(x0,a,b,c)
p<-prod(p^resp*(1-p)^(1-resp))
prior_density <- dnorm(x0, mean = mu, sd = sigma)
# n[ii]<-x0*dnorm(x0)*p
n[ii]<-x0*prior_density*p
d[ii]<-dnorm(x0)*p
}
est.eap[i]<-sum(n)/sum(d)
}
for (i in 1:length(th)) {
##we'll use th to simulate data
p<-get_pr(th[i],a,b,c)
resp<-rbinom(length(p),1,p)
##old MLE estimation based on optimization of f1
est.mle[i]<-optim(0,f1,method='Brent',lower=-10,upper=10,a=a,b=b,c=c,resp=resp)$par
##EAP estimation!!
x<-seq(-5,5,length.out=1000)
n<-d<-numeric() #numerator and deminator here  ##https://www.rasch.org/rmt/rmt163i.htm
for (ii in 1:length(x)) {
x0<-x[ii]
p<-get_pr(x0,a,b,c)
p<-prod(p^resp*(1-p)^(1-resp))
prior_density <- dnorm(x0, mean = 0, sd = 2)
# n[ii]<-x0*dnorm(x0)*p
n[ii]<-x0*prior_density*p
d[ii]<-dnorm(x0)*p
}
est.eap[i]<-sum(n)/sum(d)
}
df<-data.frame(th=th,est.mle=est.mle,est.eap=est.eap)
plot(df); abline(0,1)
nm <- c("enem_2013_1mil_ch")
dataset <- redivis::user("datapages")$dataset("item_response_warehouse")
df <- dataset$table(nm)$to_data_frame()
Sys.setenv(REDIVIS_API_TOKEN="AAACl7Hq7D275uz98AZ3FFj3U7TbiKM2")
ids<-sample(unique(df$id),50000)
df <- dataset$table(nm)$to_data_frame()
ids<-sample(unique(df$id),50000)
df<-df[df$id %in% ids,]
table(df$item,df$booklet) #just confirming items are repeated across booklet
L<-split(df,df$booklet)
f<-function(x) {
library(mirt)
x<-irw::long2resp(x)
x$id<-NULL
m<-mirt(x,1,'Rasch')
z<-coef(m,simplify=TRUE)$items
data.frame(item=rownames(z),b=z[,2])
}
est<-lapply(L,f)
x0<-est[[1]]
names(x0)[2]<-names(est)[1]
for (i in 2:length(est)) {
x<-est[[i]]
names(x)[2]<-names(est)[i]
x0<-merge(x0,x)
}
plot(x0[,-1])
plot(x0[,-1]) ; abline(0,1)
plot(x0[,-1]; abline(0,1))
plot(x0[,-1]) ; abline(0,1)
###############################################################################################
library(mirt)
library("lme4")
data("VerbAgg")
VerbAgg$r2<-ifelse(VerbAgg$r2=="Y",1,0)
VerbAgg
###############################################################################################
## A first example: A Rasch model with fixed item effects and random person effects
#m1.true<-glmer(r2 ~ 0 + item + (1|id), data=VerbAgg, family="binomial") ##takes too long!
m1<-lmer(r2 ~ 0 + item + (1|id), data=VerbAgg)
summary(m1) #what do we make of the fixed effect estimates? what are these effectively?
hist(ranef(m1)$id[,1]) #and what are these?
###############################################################################################
##mirt version
#rearrange data
L<-list()
for (i in unique(VerbAgg$item)) {
z<-VerbAgg[VerbAgg$item==i,]
L[[i]]<-z$r2[order(z$id)]
}
resp<-do.call("cbind",L)
m1.mirt<-mirt(resp,1,'Rasch')
gender<-z$Gender #saving for later use
table(gender) #what do you think about this?
###############################################################################################
##let's compare mirt and lmer estimates
par(mfrow=c(1,2),mgp=c(2,1,0),mar=c(3,3,1,1),oma=rep(.5,4))
##compare item parameters
co<-coef(m1.mirt)[-length(coef(m1.mirt))]
co<-do.call("rbind",co)
easy.pars<-cbind(fixef(m1),co[,2])
plot(easy.pars,xlab="lmer parameters",ylab="mirt parameter",pch=19)
##compare person parameters
person.pars<-cbind(ranef(m1)$id[,1],fscores(m1.mirt)[,1])
plot(jitter(person.pars[,1]),jitter(person.pars[,2]),xlab="lmer abilities",ylab="mirt abilities",cex=.5) #note the differences in the scales! what's going on?
##Is endorsement that you "want" to do something more likely than saying you'd actually "do" it?
VerbAgg$subitem<-paste(VerbAgg$btype,substr(VerbAgg$item,1,2))
m2<-lmer(r2 ~ 0+mode+subitem+ (1|id), data=VerbAgg)
summary(m2) ##what do you think?
##Are males more likely to be verbally aggressive?
m3<-lmer(r2 ~ 0+item+ Gender+(1|id), data=VerbAgg)
summary(m3)
##FWIW, the IRT-ish way of doing this is captured via the multipleGroup function in mirt
m<-multipleGroup(resp,1,group=gender,invariance=c("slopes","intercepts","free_mean","free_var"))
coef(m)$F$GroupPars
coef(m)$M$GroupPars
##Are males differentially more likely to say that they will "do" (as opposed to the gender difference for "want")?
m4<-lmer(r2 ~ 0+mode*Gender+subitem+ (1|id), data=VerbAgg)
summary(m4)
##Is endorsement that you "want" to do something more likely than saying you'd actually "do" it?
VerbAgg$subitem<-paste(VerbAgg$btype,substr(VerbAgg$item,1,2))
m2<-lmer(r2 ~ 0+mode+subitem+ (1|id), data=VerbAgg)
summary(m2) ##what do you think?
resp
##Are males more likely to be verbally aggressive?
m3<-lmer(r2 ~ 0+item+ Gender+(1|id), data=VerbAgg)
summary(m3)
##FWIW, the IRT-ish way of doing this is captured via the multipleGroup function in mirt
m<-multipleGroup(resp,1,group=gender,invariance=c("slopes","intercepts","free_mean","free_var"))
coef(m)$F$GroupPars
coef(m)$M$GroupPars
##Are males differentially more likely to say that they will "do" (as opposed to the gender difference for "want")?
m4<-lmer(r2 ~ 0+mode*Gender+subitem+ (1|id), data=VerbAgg)
summary(m4)
sim<-function(ni=30,np=2000) {
##This will simulate response data in a long format to emphasize that this is the 'likelihood' view of the response data
x<-expand.grid(id=1:np,item=1:ni)
th<-rnorm(np)
x<-merge(x,data.frame(id=1:np,th=th))
a<-exp(rnorm(ni,sd=.3))
b<-rnorm(ni)
x<-merge(x,data.frame(item=1:ni,a=a,b=b))
k<-x$th-x$b
p<-1/(1+exp(-x$a*k))
x$resp<-rbinom(length(p),1,p)
x
}
sim<-function(ni=30,np=2000) {
##This will simulate response data in a long format to emphasize that this is the 'likelihood' view of the response data
x<-expand.grid(id=1:np,item=1:ni)
th<-rnorm(np)
x<-merge(x,data.frame(id=1:np,th=th))
a<-exp(rnorm(ni,sd=.3))
b<-rnorm(ni)
x<-merge(x,data.frame(item=1:ni,a=a,b=b))
k<-x$th-x$b
p<-1/(1+exp(-x$a*k))
x$resp<-rbinom(length(p),1,p)
x
}
x<-sim()
##Now we'll induce purely random missingness
x$resp.full<-x$resp
p<-.1
miss<-rbinom(nrow(x),1,p)
x$resp<-ifelse(miss==1,NA,x$resp.full)
##now we're ready for analysis
resp<-irw::long2resp(x)
id<-resp$id #might need this!
resp$id<-NULL
library(mirt)
m<-mirt(resp,1,'2PL')
##Your task
##1. decide what you are going to analyze (options: abilities. item parameters.)
coef(m, IRTpars = TRUE, simplify = TRUE)
##Your task
##1. decide what you are going to analyze (options: abilities. item parameters.)
params <- data.frame(coef(m, IRTpars = TRUE, simplify = TRUE)
##Your task
##1. decide what you are going to analyze (options: abilities. item parameters.)
params <- data.frame(coef(m, IRTpars = TRUE, simplify = TRUE))
x$resp.full
x
x<-sim()
x
resp.original<-irw::long2resp(x)
##Now we'll induce purely random missingness
x$resp.full<-x$resp
p<-.1
miss<-rbinom(nrow(x),1,p)
x$resp<-ifelse(miss==1,NA,x$resp.full)
##now we're ready for analysis
resp<-irw::long2resp(x)
id<-resp$id #might need this!
resp$id<-NULL
library(mirt)
m<-mirt(resp,1,'2PL')
m.original<-mirt(resp.original,1,'2PL')
resp.original
resp.original<-irw::long2resp(x$resp)
x<-sim()
resp.original<-irw::long2resp(x$resp)
##Now we'll induce purely random missingness
x$resp.full<-x$resp
resp.original<-irw::long2resp(x)
resp.original$id<-NULL
resp.original<-irw::long2resp(x)
resp.original$id<-NULL
##Now we'll induce purely random missingness
x$resp.full<-x$resp
p<-.1
miss<-rbinom(nrow(x),1,p)
x$resp<-ifelse(miss==1,NA,x$resp.full)
##now we're ready for analysis
resp<-irw::long2resp(x)
id<-resp$id #might need this!
resp$id<-NULL
library(mirt)
m<-mirt(resp,1,'2PL')
m.original<-mirt(resp.original,1,'2PL')
##Your task
##1. decide what you are going to analyze (options: abilities. item parameters.)
params <- data.frame(coef(m, IRTpars = TRUE, simplify = TRUE))
params.original <- data.frame(coef(m.original, IRTpars = TRUE, simplify = TRUE))
plot(params$items.a, params.original$items.a)
p<-.5
miss<-rbinom(nrow(x),1,p)
x$resp<-ifelse(miss==1,NA,x$resp.full)
##now we're ready for analysis
resp<-irw::long2resp(x)
id<-resp$id #might need this!
resp$id<-NULL
library(mirt)
m<-mirt(resp,1,'2PL')
m.original<-mirt(resp.original,1,'2PL')
##Your task
##1. decide what you are going to analyze (options: abilities. item parameters.)
params <- data.frame(coef(m, IRTpars = TRUE, simplify = TRUE))
params.original <- data.frame(coef(m.original, IRTpars = TRUE, simplify = TRUE))
plot(params$items.a, params.original$items.a)
plot(params$items.b, params.original$items.b)
plot(params$items.a, params.original$items.a)
sim<-function(ni=30,np=2000) {
##This will simulate response data in a long format to emphasize that this is the 'likelihood' view of the response data
x<-expand.grid(id=1:np,item=1:ni)
th<-rnorm(np)
x<-merge(x,data.frame(id=1:np,th=th))
a<-exp(rnorm(ni,sd=.3))
b<-rnorm(ni)
x<-merge(x,data.frame(item=1:ni,a=a,b=b))
k<-x$th-x$b
x$p<-1/(1+exp(-x$a*k))
x$resp<-rbinom(length(x$p),1,x$p)
x
}
x<-sim()
##Now we'll induce missinness
x$resp.full<-x$resp
mmp<-.3 #this will be the Max Missing Probability.
p2<-mmp-x$p*mmp
miss<-rbinom(nrow(x),1,p2)
x$resp<-ifelse(miss==1,NA,x$resp.full)
##now we're ready for analysis
resp<-irw::long2resp(x)
id<-resp$id #might need this!
resp$id<-NULL
library(mirt)
m<-mirt(resp,1,'2PL')
x<-sim()
##Now we'll induce missinness
x$resp.full<-x$resp
resp.original<-irw::long2resp(x)
resp.original$id<-NULL
mmp<-.3 #this will be the Max Missing Probability.
p2<-mmp-x$p*mmp
miss<-rbinom(nrow(x),1,p2)
x$resp<-ifelse(miss==1,NA,x$resp.full)
##now we're ready for analysis
resp<-irw::long2resp(x)
id<-resp$id #might need this!
resp$id<-NULL
library(mirt)
m<-mirt(resp,1,'2PL')
m.original<-mirt(resp.original,1,'2PL')
##Now you need to implement your analysis
params <- data.frame(coef(m, IRTpars = TRUE, simplify = TRUE))
params.original <- data.frame(coef(m.original, IRTpars = TRUE, simplify = TRUE))
plot(params$items.a, params.original$items.a)
plot(params$items.b, params.original$items.b)
x<-sim()
##Now we'll induce missinness
x$resp.full<-x$resp
resp.original<-irw::long2resp(x)
resp.original$id<-NULL
mmp<-.3 #this will be the Max Missing Probability.
p2<-mmp-x$p*mmp
miss<-rbinom(nrow(x),1,p2)
x$resp<-ifelse(miss==1,NA,x$resp.full)
##now we're ready for analysis
resp<-irw::long2resp(x)
id<-resp$id #might need this!
resp$id<-NULL
library(mirt)
m<-mirt(resp,1,'2PL')
m.original<-mirt(resp.original,1,'2PL')
##Now you need to implement your analysis
params <- data.frame(coef(m, IRTpars = TRUE, simplify = TRUE))
params.original <- data.frame(coef(m.original, IRTpars = TRUE, simplify = TRUE))
plot(params$items.a, params.original$items.a)
plot(params$items.b, params.original$items.b)
x$p
